Networked programs
==================

Habang marami sa mga halimbawa sa libro na ito ay nakatuon sa pagbabasa ng files
at paghahanap ng data sa mga files na iyon, mayroong maraming iba't ibang sources ng 
impormasyon kapag isinasaalang-alang ng isa ang Internet.

Sa chapter na ito magpapanggap tayo bilang web browser at kukuha ng web
pages gamit ang Hypertext Transfer Protocol (HTTP). Pagkatapos magbabasa tayo
sa web page data at i-parse ito.

Hypertext Transfer Protocol - HTTP
-----------------------------------

Ang network protocol na nagpapagana sa web ay talagang medyo simple at
may built-in support sa Python na tinatawag na `socket` na
nagpapadali sa paggawa ng network connections at pagkuha ng data sa pamamagitan ng
mga sockets na iyon sa Python program.

Ang *socket* ay halos katulad ng file, maliban sa isang
socket ay nagbibigay ng two-way connection sa pagitan ng dalawang programs. Maaari mong pareho
magbasa mula at sumulat sa parehong socket. Kung sumusulat ka ng isang bagay sa
socket, ipinapadala ito sa application sa kabilang dulo ng socket. Kung
nagbabasa ka mula sa socket, binibigyan ka ng data na ipinadala ng iba pang
application.

Pero kung susubukan mong magbasa ng socket^[Kung gusto mong matuto pa tungkol sa
sockets, protocols o kung paano nabubuo ang web servers, maaari mong i-explore
ang course sa https://www.dj4e.com.] kapag ang program sa kabilang dulo ng
socket ay hindi pa nagpadala ng anumang data, nakaupo ka lang at naghihintay. Kung ang programs sa
parehong dulo ng socket ay simpleng naghihintay ng data nang hindi nagpapadala ng
anumang bagay, maghihintay sila ng napakatagal, kaya mahalagang parte ng programs
na nakikipag-communicate sa Internet ay magkaroon ng ilang uri ng protocol.

Ang protocol ay set ng tumpak na rules na
nagde-determine kung sino ang uunang pumunta, kung ano ang gagawin nila, at pagkatapos kung ano ang
mga responses sa mensaheng iyon, at sino ang susunod na magpapadala, at iba pa. Sa isang diwa
ang dalawang applications sa magkabilang dulo ng socket ay gumagawa ng sayaw at
sinisiguro na hindi tumapak sa paa ng isa't isa.

Mayroong maraming dokumento na naglalarawan sa mga network protocols na ito. Ang
Hypertext Transfer Protocol ay inilarawan sa sumusunod na dokumento:

<https://www.w3.org/Protocols/rfc2616/rfc2616.txt>

Ito ay mahabang at kumplikadong 176-page na dokumento na may maraming detalye. Kung
nakakainteresante ito sa iyo, huwag mag-atubiling basahin ang lahat. Pero kung titingnan mo
ang page 36 ng RFC2616 makikita mo ang syntax para sa GET
request. Para humingi ng dokumento mula sa web server, gumagawa tayo ng connection,
hal. sa `www.pr4e.org` server sa port 80, at pagkatapos nagpapadala ng
linya ng form

`GET http://data.pr4e.org/romeo.txt HTTP/1.0 `

kung saan ang pangalawang parameter ay ang web page na hinihingi natin, at pagkatapos
nagpapadala din tayo ng blank line. Ang web server ay magre-respond ng ilang header
impormasyon tungkol sa dokumento at blank line na sinusundan ng dokumento
content.

The world's simplest web browser
--------------------------------

Marahil ang pinakamadaling paraan para ipakita kung paano gumagana ang HTTP protocol ay sumulat ng
napakasimpleng Python program na gumagawa ng connection sa web server at
sumusunod sa rules ng HTTP protocol para humingi ng dokumento at
ipakita ang ipinadala ng server.

\VerbatimInput{../code3/socket1.py}

Una ang program ay gumagawa ng connection sa port 80 sa server
[www.pr4e.com](http://www.pr4e.com). Dahil ang program natin ay gumaganap bilang
"web browser", ang HTTP protocol ay nagsasabi na dapat nating ipadala ang GET
command na sinusundan ng blank line. Ang `\r\n` ay nangangahulugang EOL (end of line),
kaya ang `\r\n\r\n` ay nangangahulugang walang anuman sa pagitan ng dalawang EOL sequences. Iyon ang
katumbas ng blank line.

![A Socket Connection](height=2.0in@../../../images/socket)

Kapag naipadala na natin ang blank line na iyon, sumusulat tayo ng loop na tumatanggap ng data sa
512-character chunks mula sa socket at nagpi-print ng data hanggang wala nang
data na mababasa (i.e., ang recv() ay nagre-return ng empty string).

Ang program ay gumagawa ng sumusunod na output:

~~~~
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 18:52:55 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Sat, 13 May 2017 11:22:22 GMT
ETag: "a7-54f6609245537"
Accept-Ranges: bytes
Content-Length: 167
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: text/plain

But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
~~~~

Ang output ay nagsisimula sa headers na ipinapadala ng web server para ilarawan
ang dokumento. Halimbawa, ang `Content-Type` header
ay nagpapahiwatig na ang dokumento ay plain text document
(`text/plain`).

Pagkatapos ipadala sa atin ng server ang headers, nagdaragdag ito ng blank line para ipahiwatig
ang dulo ng headers, at pagkatapos nagpapadala ng aktwal na data ng file
*romeo.txt*.

Ang halimbawang ito ay nagpapakita kung paano gumawa ng low-level network connection gamit ang
sockets. Ang sockets ay maaaring gamitin para makipag-communicate sa web server o sa
mail server o marami pang ibang uri ng servers. Ang kailangan lang ay
hanapin ang dokumento na naglalarawan sa protocol at sumulat ng code para
magpadala at tumanggap ng data ayon sa protocol.

Gayunpaman, dahil ang protocol na pinakakaraniwang ginagamit natin ay ang HTTP web
protocol, ang Python ay may espesyal na library na partikular na idinisenyo para suportahan
ang HTTP protocol para sa pagkuha ng mga dokumento at data sa web.

Isa sa mga requirements para gamitin ang HTTP protocol ay ang pangangailangan na magpadala
at tumanggap ng data bilang bytes objects, sa halip na strings. Sa naunang
halimbawa, ang `encode()` at `decode()` methods ay nagko-convert ng strings sa bytes
objects at pabalik.

Ang susunod na halimbawa ay gumagamit ng `b''` notation para tukuyin na ang variable ay dapat
i-store bilang bytes object. Ang `encode()` at `b''` ay katumbas.

~~~~
>>> b'Hello world'
b'Hello world'
>>> 'Hello world'.encode()
b'Hello world'
~~~~

Retrieving an image over HTTP
-----------------------------

\index{urllib!image}
\index{image!jpg}
\index{jpg}

Sa halimbawa sa itaas, kumuha tayo ng plain text file na may newlines
sa file at simpleng kinopya ang data sa screen habang tumatakbo ang program.
Maaari tayong gumamit ng katulad na program para kumuha ng image gamit ang
HTTP. Sa halip na kopyahin ang data sa screen habang tumatakbo ang program, 
ina-accumulate natin ang data sa string, tinatanggal ang headers, at pagkatapos sinasave ang
image data sa file tulad ng sumusunod:

\VerbatimInput{../code3/urljpeg.py}

Kapag tumatakbo ang program, gumagawa ito ng sumusunod na output:

~~~~
$ python urljpeg.py
5120 5120
5120 10240
4240 14480
5120 19600
...
5120 214000
3200 217200
5120 222320
5120 227440
3167 230607
Header length 393
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 18:54:09 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
~~~~

Makikita mo na para sa url na ito, ang `Content-Type` header
ay nagpapahiwatig na ang body ng dokumento ay image
(`image/jpeg`). Kapag natapos na ang program, maaari mong tingnan ang
image data sa pamamagitan ng pagbubukas ng file na `stuff.jpg` sa image
viewer.

Habang tumatakbo ang program, makikita mo na hindi tayo nakakakuha ng 5120 characters sa bawat
pagkakataon na tinatawag natin ang `recv()` method. Nakakakuha tayo ng kasing dami ng characters
na na-transfer sa network sa atin ng web server sa
sandaling tinatawag natin ang `recv()`. Sa halimbawang ito, nakakakuha tayo ng
kasing kaunti ng 3200 characters sa bawat pagkakataon na humihingi tayo ng hanggang 5120 characters ng
data.

Ang iyong results ay maaaring magkaiba depende sa network speed mo. Tandaan din
na sa huling tawag sa `recv()` nakakakuha tayo ng 3167 bytes, na siyang
dulo ng stream, at sa susunod na tawag sa `recv()` 
nakakakuha tayo ng zero-length string na nagsasabi sa atin na ang server ay tumawag na sa
`close()` sa dulo nito ng socket at wala nang data na
darating.

\index{time}
\index{time.sleep}

Maaari nating pabagalin ang sunud-sunod na `recv()` calls natin sa pamamagitan ng
pag-uncomment sa tawag sa `time.sleep()`. Sa ganitong paraan, naghihintay tayo ng
quarter ng segundo pagkatapos ng bawat tawag para makakuha ng "lead" ang server
at magpadala ng mas maraming data sa atin bago tayo tumawag ulit sa `recv()`.
Sa delay, sa lugar ang program ay nag-e-execute tulad ng sumusunod:

~~~~
$ python urljpeg.py
5120 5120
5120 10240
5120 15360
...
5120 225280
5120 230400
207 230607
Header length 393
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 21:42:08 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
~~~~

Ngayon maliban sa una at huling tawag sa `recv()`, nakakakuha na tayo ng
5120 characters sa bawat pagkakataon na humihingi tayo ng bagong data.

Mayroong buffer sa pagitan ng server na gumagawa ng `send()` requests
at application natin na gumagawa ng `recv()` requests. Kapag pinatakbo natin ang
program na may delay sa lugar, sa ilang punto ang server ay maaaring mapuno ang
buffer sa socket at mapilit na mag-pause hanggang magsimulang mag-emptying ng buffer ang program natin.
Ang pag-pause ng alinman sa sending application o
receiving application ay tinatawag na "flow control."

\index{flow control}

Retrieving web pages with `urllib`
---------------------------------------------

Habang maaari nating manu-manong magpadala at tumanggap ng data sa HTTP gamit ang socket
library, mayroong mas simpleng paraan para gawin ang karaniwang gawaing ito sa
Python sa pamamagitan ng paggamit ng `urllib` library.

Gamit ang `urllib`, maaari mong tratuhin ang web page na parang file.
Simpleng ipinapahiwatig mo lang kung aling web page ang gusto mong kunin at
ang `urllib` ay nagha-handle ng lahat ng HTTP protocol at header details.

Ang katumbas na code para basahin ang *romeo.txt* file mula sa web
gamit ang `urllib` ay ganito:

\VerbatimInput{../code3/urllib1.py}

Kapag nabuksan na ang web page gamit ang `urllib.request.urlopen`, maaari nating
tratuhin ito na parang file at basahin ito gamit ang `for`
loop.

Kapag tumatakbo ang program, nakikita lang natin ang output ng contents ng
file. Ang headers ay ipinapadala pa rin, pero ang `urllib` code
ay kumukonsumo ng headers at nagre-return lang ng data sa atin.

~~~~
But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
~~~~

Bilang halimbawa, maaari tayong sumulat ng program para kunin ang data para sa
`romeo.txt` at i-compute ang frequency ng bawat salita sa
file tulad ng sumusunod:

\VerbatimInput{../code3/urlwords.py}

Muli, kapag nabuksan na natin ang web page, maaari nating basahin ito na parang local
file.

Reading binary files using `urllib`
-----------------------------------

Minsan gusto mong kunin ang non-text (o binary) file tulad ng
image o video file. Ang data sa mga files na ito ay karaniwang hindi kapaki-pakinabang na
i-print, pero madali mong magagawa ng kopya ng URL sa local file sa
hard disk mo gamit ang `urllib`.

\index{binary file}

Ang pattern ay buksan ang URL at gamitin ang `read` para i-download ang
buong contents ng dokumento sa string variable
(`img`) pagkatapos isulat ang impormasyong iyon sa local file tulad ng
sumusunod:

\VerbatimInput{../code3/curl1.py}

Ang program na ito ay nagbabasa ng lahat ng data nang sabay-sabay sa network at
nag-i-store nito sa variable na `img` sa main memory ng iyong
computer, pagkatapos binubuksan ang file na `cover.jpg` at isinusulat ang data
sa disk mo. Ang `wb` argument para sa `open()` ay nagbubukas ng binary file
para sa pagsusulat lang. Ang program na ito ay gagana kung ang laki ng file ay mas maliit kaysa sa
laki ng memory ng computer mo.

Gayunpaman kung ito ay malaking audio o video file, ang program na ito ay maaaring mag-crash
o hindi bababa ay tumakbo nang napakabagal kapag naubusan ng memory ang computer mo.
Para maiwasan ang pagkaubos ng memory, kinukuha natin ang data sa blocks
(o buffers) at pagkatapos isinusulat ang bawat block sa disk mo bago kunin ang
susunod na block. Sa ganitong paraan ang program ay maaaring magbasa ng anumang laking file nang hindi
ginagamit ang lahat ng memory na mayroon ka sa computer mo.

\VerbatimInput{../code3/curl2.py}

Sa halimbawang ito, nagbabasa lang tayo ng 100,000 characters sa isang pagkakataon at pagkatapos
isinusulat ang mga characters na iyon sa file na `cover3.jpg` bago
kunin ang susunod na 100,000 characters ng data mula sa web.

Ang program na ito ay tumatakbo tulad ng sumusunod:

~~~~
python curl2.py
230210 characters copied.
~~~~

Parsing HTML and scraping the web
---------------------------------

\index{web!scraping}
\index{parsing HTML}

Isa sa karaniwang gamit ng `urllib` capability sa Python
ay *i-scrape* ang web. Ang web scraping ay kapag sumusulat tayo ng
program na nagpapanggap bilang web browser at kumukuha ng pages, pagkatapos
sinusuri ang data sa mga pages na iyon na naghahanap ng patterns.

Bilang halimbawa, ang search engine tulad ng Google ay titingnan ang source ng
isang web page at kukunin ang links sa iba pang pages at kukunin ang mga
pages na iyon, kumukuha ng links, at iba pa. Gamit ang technique na ito, ang Google ay
*nag-spider* sa halos lahat ng pages sa
web.

Gumagamit din ang Google ng frequency ng links mula sa pages na nakikita nito patungo sa
partikular na page bilang isang sukat kung gaano "important" ang page at kung gaano kataas
dapat lumabas ang page sa search results nito.

Parsing HTML using regular expressions
--------------------------------------

Ang isang simpleng paraan para mag-parse ng HTML ay gumamit ng regular expressions para paulit-ulit na
maghanap at kunin ang substrings na tumutugma sa partikular na pattern.

Narito ang simpleng web page:

~~~~ {.html}
<h1>The First Page</h1>
<p>
If you like, you can switch to the
<a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>.
</p>
~~~~

Maaari tayong gumawa ng well-formed regular expression para tumugma at kunin
ang link values mula sa text sa itaas tulad ng sumusunod:

~~~~
href="http[s]?://.+?"
~~~~

Ang regular expression natin ay naghahanap ng strings na nagsisimula sa
"href=\"http://" o "href=\"https://", na sinusundan ng isa o higit pang characters (`.+?`),
na sinusundan ng isa pang double quote. Ang question mark sa likod ng `[s]?` ay nagpapahiwatig
na maghanap ng string na "http" na sinusundan ng zero o isang "s". 

Ang question mark na idinagdag sa `.+?` ay nagpapahiwatig
na ang match ay dapat gawin sa "non-greedy" fashion sa halip na
"greedy" fashion. Ang non-greedy match ay sinusubukang hanapin ang
*pinakamaliit* na posibleng matching string at ang greedy match
ay sinusubukang hanapin ang *pinakamalaki* na posibleng matching string.

\index{greedy}
\index{non-greedy}

Nagdaragdag tayo ng parentheses sa regular expression natin para ipahiwatig kung aling parte ng
matched string natin ang gusto nating kunin, at gumagawa ng sumusunod
na program:

\index{regex!parentheses}
\index{parentheses!regular expression}

\VerbatimInput{../code3/urlregex.py}

Ang `ssl` library ay nagpapahintulot sa program na ito na ma-access ang web sites na mahigpit na
nagpapatupad ng HTTPS. Ang `read` method ay nagre-return ng HTML source code bilang bytes object 
sa halip na mag-return ng HTTPResponse object. Ang `findall` regular expression
method ay magbibigay sa atin ng list ng lahat ng strings na tumutugma sa aming
regular expression, na nagre-return lang ng link text sa pagitan ng double quotes.

Kapag pinatakbo natin ang program at nag-input ng URL, makukuha natin ang sumusunod na output:

~~~~
Enter - https://docs.python.org
https://docs.python.org/3/index.html
https://www.python.org/
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
https://www.python.org/
https://www.python.org/psf/donations/
http://sphinx.pocoo.org/
~~~~

Ang regular expressions ay gumagana nang napakaganda kapag ang HTML mo ay well formatted
at predictable. Pero dahil mayroong maraming "broken" HTML pages doon,
ang solusyon na gumagamit lang ng regular expressions ay maaaring makaligtaan ang ilang
valid links o magtapos sa masamang data.

Maaari itong malutas sa pamamagitan ng paggamit ng robust HTML parsing library.

Parsing HTML using BeautifulSoup
--------------------------------

\index{BeautifulSoup}

Kahit na ang HTML ay mukhang XML^[Ang XML format ay inilarawan sa susunod na chapter.]
at ang ilang pages ay maingat na
ginawa para maging XML, karamihan ng HTML ay karaniwang sira sa mga paraan na nagdudulot
na ang XML parser ay tumanggi sa buong page ng HTML bilang hindi wastong format.

Mayroong ilang Python libraries na maaaring tumulong sa iyo na mag-parse ng HTML at
kumuha ng data mula sa pages. Ang bawat library ay may sariling strengths at
weaknesses at maaari kang pumili ng isa batay sa iyong pangangailangan.

Bilang halimbawa, simpleng magpa-parse tayo ng ilang HTML input at kumuha ng links
gamit ang *BeautifulSoup* library. Ang BeautifulSoup ay nagpaparaya sa lubhang flawed
HTML at nagpapahintulot pa rin sa iyo na madaling kunin ang data na kailangan mo. Maaari mong i-download at
i-install ang BeautifulSoup code mula sa:

<https://pypi.python.org/pypi/beautifulsoup4>

Ang impormasyon tungkol sa pag-install ng BeautifulSoup gamit ang Python Package Index tool na `pip`
ay available sa:

<https://packaging.python.org/tutorials/installing-packages/>

Gagamitin natin ang `urllib` para basahin ang page at pagkatapos gamitin ang
`BeautifulSoup` para kunin ang `href` attributes
mula sa anchor (`a`) tags.

\index{BeautifulSoup}
\index{HTML}
\index{parsing!HTML}

\VerbatimInput{../code3/urllinks.py}

Ang program ay nagpo-prompt para sa web address, pagkatapos binubuksan ang web page, binabasa
ang data at ipinapasa ang data sa BeautifulSoup parser, at pagkatapos
kumukuha ng lahat ng anchor tags at nagpi-print ng `href`
attribute para sa bawat tag.

Kapag tumatakbo ang program, gumagawa ito ng sumusunod na output:

~~~~
Enter - https://docs.python.org
genindex.html
py-modindex.html
https://www.python.org/
#
whatsnew/3.6.html
whatsnew/index.html
tutorial/index.html
library/index.html
reference/index.html
using/index.html
howto/index.html
installing/index.html
distributing/index.html
extending/index.html
c-api/index.html
faq/index.html
py-modindex.html
genindex.html
glossary.html
search.html
contents.html
bugs.html
about.html
license.html
copyright.html
download.html
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
genindex.html
py-modindex.html
https://www.python.org/
#
copyright.html
https://www.python.org/psf/donations/
bugs.html
http://sphinx.pocoo.org/
~~~~

Ang list na ito ay mas mahaba dahil ang ilang HTML anchor tags ay relative
paths (hal., tutorial/index.html) o in-page references (hal., '#')
na hindi kasama ang "http://" o "https://", na siyang
requirement sa regular expression natin.

Maaari mo ring gamitin ang BeautifulSoup para kunin ang iba't ibang parte ng bawat tag:

\VerbatimInput{../code3/urllink2.py}

~~~~
python urllink2.py
Enter - http://www.dr-chuck.com/page1.htm
TAG: <a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>
URL: http://www.dr-chuck.com/page2.htm
Content: ['\nSecond Page']
Attrs: [('href', 'http://www.dr-chuck.com/page2.htm')]
~~~~

Ang `html.parser` ay ang HTML parser na kasama sa standard Python 3 library.
Ang impormasyon tungkol sa iba pang HTML parsers ay available sa:

<http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser>

Ang mga halimbawang ito ay nagsisimula lang na ipakita ang kapangyarihan ng BeautifulSoup pagdating sa
pag-parse ng HTML.

Bonus section for Unix / Linux users
------------------------------------

Kung mayroon kang Linux, Unix, o Macintosh computer, malamang mayroon kang
commands na naka-build sa operating system mo na kumukuha ng parehong plain
text at binary files gamit ang HTTP o File Transfer (FTP) protocols.
Isa sa mga commands na ito ay `curl`:

\index{curl}

~~~~ {.bash}
$ curl -O http://www.py4e.com/cover.jpg
~~~~

Ang command na `curl` ay maikli para sa "copy URL" at kaya ang dalawang
halimbawa na nakalista kanina para kumuha ng binary files gamit ang `urllib`
ay matalinong pinangalanan na `curl1.py` at `curl2.py` sa
[www.py4e.com/code3](http://www.py4e.com/code3) dahil
nag-i-implement sila ng katulad na functionality sa `curl` command.
Mayroon ding `curl3.py` sample program na gumagawa ng gawaing ito
nang mas epektibo, kung sakaling gusto mong talagang gamitin ang pattern na ito
sa program na sinusulat mo.

Ang pangalawang command na gumagana nang katulad ay `wget`:

\index{wget}

~~~~ {.bash}
$ wget http://www.py4e.com/cover.jpg
~~~~

Pareho sa mga commands na ito ay nagpapasimple sa pagkuha ng webpages at remote files.

Glossary
--------

BeautifulSoup
:   Python library para sa pag-parse ng HTML documents at pagkuha ng data mula sa
    HTML documents na nagko-compensate para sa karamihan ng imperfections sa
    HTML na karaniwang hindi pinapansin ng browsers. Maaari mong i-download ang
    BeautifulSoup code mula sa [www.crummy.com](http://www.crummy.com).
\index{BeautifulSoup}

port
:   Numero na karaniwang nagpapahiwatig kung aling application ang
    kinokontak mo kapag gumagawa ka ng socket connection sa server. Bilang
    halimbawa, ang web traffic ay karaniwang gumagamit ng port 80 habang ang email traffic ay gumagamit ng
    port 25.
\index{port}

scrape
:   Kapag ang program ay nagpapanggap bilang web browser at kumukuha ng web
    page, pagkatapos tumitingin sa web page content. Kadalasan ang mga programs ay
    sumusunod sa links sa isang page para hanapin ang susunod na page para maaari nilang
    dumaan sa network ng pages o social network.
\index{socket}

socket
:   Network connection sa pagitan ng dalawang applications kung saan ang applications
    ay maaaring magpadala at tumanggap ng data sa alinmang direksyon.
\index{socket}

spider
:   Ang gawain ng web search engine na kumukuha ng page at pagkatapos lahat ng
    pages na naka-link mula sa page at iba pa hanggang mayroon na silang halos lahat ng
    pages sa Internet na ginagamit nila para gumawa ng search index nila.
\index{spider}

Exercises
---------

**Exercise 1:** Baguhin ang socket program na `socket1.py` para mag-prompt
sa user para sa URL para makabasa ito ng anumang web page.

Maaari mong gamitin ang
`split('/')` para hatiin ang URL sa component parts nito para
makakuha ka ng host name para sa socket `connect` call. Magdagdag ng
error checking gamit ang `try` at `except` para ma-handle
ang kondisyon kung saan ang user ay nag-e-enter ng hindi wastong format o
hindi umiiral na URL.

**Exercise 2:** Baguhin ang socket program mo para bilangin ang bilang ng
characters na natanggap nito at huminto sa pag-display ng anumang text pagkatapos na
ipakita ang 3000 characters. Ang program ay dapat kumuha ng buong dokumento
at bilangin ang kabuuang bilang ng characters at ipakita ang bilang ng
bilang ng characters sa dulo ng dokumento.

**Exercise 3:** Gumamit ng `urllib` para i-replicate ang naunang exercise
ng (1) pagkuha ng dokumento mula sa URL, (2) pag-display ng hanggang 3000
characters, at (3) pagbibilang ng kabuuang bilang ng characters sa
dokumento. Huwag mag-alala tungkol sa headers para sa exercise na ito, simpleng ipakita
ang unang 3000 characters ng document contents.

**Exercise 4:** Baguhin ang `urllinks.py` program para kunin at
bilangin ang paragraph (p) tags mula sa retrieved HTML document at ipakita
ang bilang ng paragraphs bilang output ng program mo. Huwag
ipakita ang paragraph text, bilangin lang sila. I-test ang program mo sa
ilang maliliit na web pages pati na rin sa ilang mas malalaking web pages.

**Exercise 5:** (Advanced) Baguhin ang socket program para ipakita lang
ang data pagkatapos matanggap ang headers at blank line. Tandaan
na ang `recv` ay tumatanggap ng characters (newlines at lahat), hindi lines.

